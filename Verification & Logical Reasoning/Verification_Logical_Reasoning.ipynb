{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOD71vgahu6mrdQdSXchC0p"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "\n",
        "!pip install -q --upgrade transformers datasets accelerate\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3Mh2xJ4h9OGK",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1759946423360,
          "user_tz": -330,
          "elapsed": 16503,
          "user": {
            "displayName": "Harshita Gupta",
            "userId": "04157109110530764624"
          }
        },
        "outputId": "6c235070-3264-46e3-93a1-5b43657831b2"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m503.6/503.6 kB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m42.8/42.8 MB\u001b[0m \u001b[31m17.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "cudf-cu12 25.6.0 requires pyarrow<20.0.0a0,>=14.0.0; platform_machine == \"x86_64\", but you have pyarrow 21.0.0 which is incompatible.\n",
            "pylibcudf-cu12 25.6.0 requires pyarrow<20.0.0a0,>=14.0.0; platform_machine == \"x86_64\", but you have pyarrow 21.0.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ==========================\n",
        "# Robust T5-small fine-tune on ProofWriter (Colab-ready)\n",
        "# Automatically handles older/newer transformers APIs\n",
        "# ==========================\n",
        "import inspect\n",
        "import os\n",
        "import pprint\n",
        "\n",
        "# Print versions for debugging\n",
        "import transformers, datasets\n",
        "print(\"transformers version:\", transformers.__version__)\n",
        "print(\"datasets version:\", datasets.__version__)\n",
        "\n",
        "from datasets import load_dataset\n",
        "from transformers import (\n",
        "    T5Tokenizer,\n",
        "    T5ForConditionalGeneration,\n",
        "    TrainingArguments,\n",
        "    Trainer,\n",
        ")\n",
        "\n",
        "# Try to import seq2seq helpers (not required, but preferred if available)\n",
        "try:\n",
        "    from transformers import Seq2SeqTrainer, Seq2SeqTrainingArguments, DataCollatorForSeq2Seq\n",
        "    _has_seq2seq = True\n",
        "except Exception:\n",
        "    Seq2SeqTrainer = None\n",
        "    Seq2SeqTrainingArguments = None\n",
        "    DataCollatorForSeq2Seq = None\n",
        "    _has_seq2seq = False\n",
        "\n",
        "# 1) Load dataset\n",
        "dataset = load_dataset(\"D3xter1922/proofwriter-dataset\")\n",
        "print(dataset)\n",
        "pprint.pprint(dataset[\"train\"][0])  # inspect one sample\n",
        "\n",
        "# 2) Tokenizer and Model\n",
        "tokenizer = T5Tokenizer.from_pretrained(\"t5-small\")\n",
        "model = T5ForConditionalGeneration.from_pretrained(\"t5-small\")\n",
        "\n",
        "# ensure tokenizer has pad token\n",
        "if tokenizer.pad_token is None:\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "# 3) Preprocessing function (handles the 'translation' nested field)\n",
        "def preprocess_function(examples):\n",
        "    # examples[\"translation\"] is a list of dicts for batched=True\n",
        "    inputs = []\n",
        "    targets = []\n",
        "\n",
        "    for item in examples[\"translation\"]:\n",
        "        # item is usually {'en': 'input text ...', 'ro': 'target ...'}\n",
        "        if isinstance(item, dict):\n",
        "            # prefer 'en' and 'ro' keys, otherwise fallback to first/second values\n",
        "            input_text = item.get(\"en\") or item.get(\"input\") or list(item.values())[0]\n",
        "            target_text = item.get(\"ro\") or item.get(\"target\") or item.get(\"output\") or (list(item.values())[1] if len(item.values()) > 1 else \"\")\n",
        "        else:\n",
        "            # defensive fallback\n",
        "            input_text = str(item)\n",
        "            target_text = \"\"\n",
        "        inputs.append(input_text)\n",
        "        targets.append(target_text)\n",
        "\n",
        "    model_inputs = tokenizer(inputs, padding=\"max_length\", truncation=True, max_length=512)\n",
        "\n",
        "    # Tokenize labels (targets) and replace pad token id's by -100 so they are ignored in loss\n",
        "    with tokenizer.as_target_tokenizer():\n",
        "        labels = tokenizer(targets, padding=\"max_length\", truncation=True, max_length=128)\n",
        "\n",
        "    label_ids = labels[\"input_ids\"]\n",
        "    # replace pad token id's with -100\n",
        "    label_ids = [[(l if l != tokenizer.pad_token_id else -100) for l in seq] for seq in label_ids]\n",
        "\n",
        "    model_inputs[\"labels\"] = label_ids\n",
        "    return model_inputs\n",
        "\n",
        "# 4) Apply preprocessing (this prints \"Map: 7%\" etc. while running)\n",
        "tokenized_datasets = dataset.map(preprocess_function, batched=True)\n",
        "\n",
        "# 5) Build training_args robustly:\n",
        "desired_training_kwargs = {\n",
        "    \"output_dir\": \"./t5-proofwriter\",\n",
        "    \"eval_strategy\": \"epoch\",           # prefer new name\n",
        "    \"learning_rate\": 3e-5,\n",
        "    \"per_device_train_batch_size\": 8,\n",
        "    \"per_device_eval_batch_size\": 8,\n",
        "    \"num_train_epochs\": 3,\n",
        "    \"weight_decay\": 0.01,\n",
        "    \"logging_dir\": \"./logs\",\n",
        "    \"logging_steps\": 100,\n",
        "    \"save_steps\": 500,\n",
        "    \"save_total_limit\": 2,\n",
        "    \"predict_with_generate\": True,      # may or may not be accepted by your installed TrainingArguments\n",
        "}\n",
        "\n",
        "# Decide which TrainingArguments class to instantiate:\n",
        "TrainingArgsClass = None\n",
        "TrainerClass = None\n",
        "\n",
        "# Prefer Seq2SeqTrainingArguments + Seq2SeqTrainer if available and supports predict_with_generate\n",
        "if _has_seq2seq:\n",
        "    try:\n",
        "        sig = inspect.signature(Seq2SeqTrainingArguments.__init__)\n",
        "        if \"predict_with_generate\" in sig.parameters:\n",
        "            TrainingArgsClass = Seq2SeqTrainingArguments\n",
        "            TrainerClass = Seq2SeqTrainer\n",
        "    except Exception:\n",
        "        pass\n",
        "\n",
        "# Fallback to plain TrainingArguments\n",
        "if TrainingArgsClass is None:\n",
        "    TrainingArgsClass = TrainingArguments\n",
        "    TrainerClass = Trainer\n",
        "\n",
        "# Filter kwargs to only those accepted by the chosen TrainingArgsClass\n",
        "init_params = inspect.signature(TrainingArgsClass.__init__).parameters\n",
        "accepted = set(init_params.keys()) - {\"self\", \"kwargs\"}\n",
        "\n",
        "# allow aliases between 'eval_strategy' <-> 'evaluation_strategy' (some versions use one name)\n",
        "aliases = {\n",
        "    \"eval_strategy\": \"evaluation_strategy\",\n",
        "    \"evaluation_strategy\": \"eval_strategy\",\n",
        "}\n",
        "\n",
        "final_kwargs = {}\n",
        "for k, v in desired_training_kwargs.items():\n",
        "    if k in accepted:\n",
        "        final_kwargs[k] = v\n",
        "    else:\n",
        "        # try alias\n",
        "        alias = aliases.get(k)\n",
        "        if alias and alias in accepted:\n",
        "            final_kwargs[alias] = v\n",
        "        else:\n",
        "            # skip unsupported arg (safe)\n",
        "            print(f\"Note: '{k}' not supported by {TrainingArgsClass.__name__}; skipping it.\")\n",
        "\n",
        "print(f\"Using TrainingArgs class: {TrainingArgsClass.__name__}\")\n",
        "print(\"Final training args:\")\n",
        "pprint.pprint(final_kwargs)\n",
        "\n",
        "training_args = TrainingArgsClass(**final_kwargs)\n",
        "\n",
        "# 6) Data collator\n",
        "data_collator = None\n",
        "if _has_seq2seq and DataCollatorForSeq2Seq is not None:\n",
        "    data_collator = DataCollatorForSeq2Seq(tokenizer, model=model)\n",
        "\n",
        "# 7) Initialize Trainer (or Seq2SeqTrainer)\n",
        "trainer_init_kwargs = {\n",
        "    \"model\": model,\n",
        "    \"args\": training_args,\n",
        "    \"train_dataset\": tokenized_datasets[\"train\"],\n",
        "    # prefer validation split for eval\n",
        "    \"eval_dataset\": tokenized_datasets.get(\"validation\", tokenized_datasets.get(\"test\", None)),\n",
        "}\n",
        "\n",
        "# attach tokenizer & data collator if available\n",
        "trainer_init_kwargs[\"tokenizer\"] = tokenizer\n",
        "if data_collator is not None:\n",
        "    trainer_init_kwargs[\"data_collator\"] = data_collator\n",
        "\n",
        "trainer = TrainerClass(**trainer_init_kwargs)\n",
        "\n",
        "# 8) Train\n",
        "trainer.train()\n",
        "\n",
        "# 9) Save\n",
        "model.save_pretrained(\"./t5-proofwriter\")\n",
        "tokenizer.save_pretrained(\"./t5-proofwriter\")\n",
        "\n",
        "# 10) Evaluate\n",
        "eval_results = trainer.evaluate()\n",
        "print(\"Evaluation Results:\", eval_results)\n",
        "\n",
        "# 11) Quick sample generation (optional)\n",
        "example_input = \"question: The tiger visits the rabbit. context: sent1: The cow is round. sent2: The cow needs the lion. sent3: The cow needs the rabbit.\"\n",
        "inputs = tokenizer(example_input, return_tensors=\"pt\", truncation=True, padding=\"longest\").to(model.device)\n",
        "with torch.no_grad():\n",
        "    out = model.generate(**inputs, max_new_tokens=64)\n",
        "gen = tokenizer.decode(out[0], skip_special_tokens=True)\n",
        "print(\"Sample generation:\", gen)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "SJo4YfEv-vqG",
        "outputId": "73d90c07-0333-43a3-ca31-21c49bee992f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "transformers version: 4.57.0\n",
            "datasets version: 4.0.0\n",
            "DatasetDict({\n",
            "    train: Dataset({\n",
            "        features: ['translation'],\n",
            "        num_rows: 41388\n",
            "    })\n",
            "    validation: Dataset({\n",
            "        features: ['translation'],\n",
            "        num_rows: 6012\n",
            "    })\n",
            "    test: Dataset({\n",
            "        features: ['translation'],\n",
            "        num_rows: 11820\n",
            "    })\n",
            "})\n",
            "{'translation': {'en': '$answer$ ; $proof$ ; $question$ = The tiger visits the '\n",
            "                       'rabbit. ; $context$ = sent1: The cow is round. sent2: '\n",
            "                       'The cow needs the lion. sent3: The cow needs the '\n",
            "                       'rabbit. sent4: The cow sees the lion. sent5: The cow '\n",
            "                       'visits the rabbit. sent6: The lion is round. sent7: '\n",
            "                       'The rabbit is kind. sent8: The rabbit visits the '\n",
            "                       'tiger. sent9: The tiger is big. sent10: The tiger is '\n",
            "                       'kind. sent11: The tiger sees the rabbit. sent12: The '\n",
            "                       'tiger visits the rabbit. sent13: If something is kind '\n",
            "                       'and it visits the rabbit then it is young. sent14: If '\n",
            "                       'something sees the tiger and it visits the lion then '\n",
            "                       'it sees the rabbit. sent15: If something is big and '\n",
            "                       'young then it sees the lion. sent16: If something '\n",
            "                       'visits the rabbit then the rabbit needs the lion. '\n",
            "                       'sent17: If something is big then it visits the rabbit. '\n",
            "                       'sent18: If something sees the tiger then it is rough. '\n",
            "                       'sent19: If something visits the rabbit and it is kind '\n",
            "                       'then the rabbit needs the lion. sent20: If something '\n",
            "                       'is rough and kind then it visits the lion. sent21: If '\n",
            "                       'something needs the lion then it is big.',\n",
            "                 'ro': '$answer$ = True ; $proof$ = sent12'}}\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/41388 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/transformers/tokenization_utils_base.py:4034: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/6012 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/11820 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using TrainingArgs class: Seq2SeqTrainingArguments\n",
            "Final training args:\n",
            "{'eval_strategy': 'epoch',\n",
            " 'learning_rate': 3e-05,\n",
            " 'logging_dir': './logs',\n",
            " 'logging_steps': 100,\n",
            " 'num_train_epochs': 3,\n",
            " 'output_dir': './t5-proofwriter',\n",
            " 'per_device_eval_batch_size': 8,\n",
            " 'per_device_train_batch_size': 8,\n",
            " 'predict_with_generate': True,\n",
            " 'save_steps': 500,\n",
            " 'save_total_limit': 2,\n",
            " 'weight_decay': 0.01}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-2542245963.py:163: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Seq2SeqTrainer.__init__`. Use `processing_class` instead.\n",
            "  trainer = TrainerClass(**trainer_init_kwargs)\n",
            "/usr/local/lib/python3.12/dist-packages/notebook/notebookapp.py:191: SyntaxWarning: invalid escape sequence '\\/'\n",
            "  | |_| | '_ \\/ _` / _` |  _/ -_)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "        window._wandbApiKey = new Promise((resolve, reject) => {\n",
              "            function loadScript(url) {\n",
              "            return new Promise(function(resolve, reject) {\n",
              "                let newScript = document.createElement(\"script\");\n",
              "                newScript.onerror = reject;\n",
              "                newScript.onload = resolve;\n",
              "                document.body.appendChild(newScript);\n",
              "                newScript.src = url;\n",
              "            });\n",
              "            }\n",
              "            loadScript(\"https://cdn.jsdelivr.net/npm/postmate/build/postmate.min.js\").then(() => {\n",
              "            const iframe = document.createElement('iframe')\n",
              "            iframe.style.cssText = \"width:0;height:0;border:none\"\n",
              "            document.body.appendChild(iframe)\n",
              "            const handshake = new Postmate({\n",
              "                container: iframe,\n",
              "                url: 'https://wandb.ai/authorize'\n",
              "            });\n",
              "            const timeout = setTimeout(() => reject(\"Couldn't auto authenticate\"), 5000)\n",
              "            handshake.then(function(child) {\n",
              "                child.on('authorize', data => {\n",
              "                    clearTimeout(timeout)\n",
              "                    resolve(data)\n",
              "                });\n",
              "            });\n",
              "            })\n",
              "        });\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize?ref=models\n",
            "wandb: Paste an API key from your profile and hit enter:"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " \u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: No netrc file found, creating one.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mharshita-gupta3021\u001b[0m (\u001b[33mharshita-gupta3021-institute-of-technology-management\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.22.1"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20251008_181149-9qrnzcty</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/harshita-gupta3021-institute-of-technology-management/huggingface/runs/9qrnzcty' target=\"_blank\">elated-bird-1</a></strong> to <a href='https://wandb.ai/harshita-gupta3021-institute-of-technology-management/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/harshita-gupta3021-institute-of-technology-management/huggingface' target=\"_blank\">https://wandb.ai/harshita-gupta3021-institute-of-technology-management/huggingface</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/harshita-gupta3021-institute-of-technology-management/huggingface/runs/9qrnzcty' target=\"_blank\">https://wandb.ai/harshita-gupta3021-institute-of-technology-management/huggingface/runs/9qrnzcty</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:666: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
            "  warnings.warn(warn_msg)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='49' max='15522' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [   49/15522 17:51 < 98:00:47, 0.04 it/s, Epoch 0.01/3]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "g2Q-GH4p_e6F"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}